## docker-compose.portainer.yml — OpenMemory production stack for Portainer / TrueNAS SCALE
##
## Deploy as a Portainer STACK (not individual container).
## Stack = Compose engine: handles multi-service ordering, volumes, health checks.
##
## ── TrueNAS SCALE notes ──────────────────────────────────────────────────────
## Uses plain named volumes — TrueNAS stores them at:
##   /mnt/.ix-apps/docker/volumes/<stackname>_memgraph-data/_data
## They survive image version updates and are visible in Portainer → Volumes.
##
## vm.max_map_count is set by the init-sysctl service before Memgraph starts.
## TrueNAS SCALE kernel default is 65530 which is too low for HNSW indexes.
## To persist across TrueNAS reboots, add to System → Advanced → Sysctl:
##   Key: vm.max_map_count   Value: 1048576
##
## ── Image build ─────────────────────────────────────────────────────────────
## Build on any machine with Docker and push to a registry, then reference below.
##   docker build -t your-registry/mem0/openmemory:latest ./openmemory/ui/
##   docker push your-registry/mem0/openmemory:latest
## Update the openmemory image: line accordingly.
##
## ── Required stack environment variables (set in Portainer "Env" tab) ──────
##   OPENAI_API_KEY          your OpenAI API key
##   OPENMEMORY_USER_ID      user slug shown in the UI (e.g. "serhii")
## Optional:
##   OPENMEMORY_PORT         host port for the UI (default 3000)
##   MEMGRAPH_PORT_BOLT      host port for Bolt (default 7687)
##   MEMGRAPH_PORT_LAB       host port for Memgraph Lab (default 7444)
## ────────────────────────────────────────────────────────────────────────────

##############################################################################
# Networks
##############################################################################
networks:
  openmemory-net:
    driver: bridge
    # Isolated internal network — only the mapped ports are exposed to the host

##############################################################################
# Volumes — plain named volumes, no bind mounts.
# TrueNAS SCALE / Portainer manages storage location automatically.
# Data persists across container restarts and image version updates.
##############################################################################
volumes:
  memgraph-data: {}
  memgraph-logs: {}
  openmemory-data: {}

##############################################################################
# Services
##############################################################################
services:

  # ── 0. One-shot init container: sets vm.max_map_count on the host ─────────
  # Memgraph HNSW vector index needs a high mmap limit.
  # This privileged init service sets it before Memgraph starts.
  # restart: "no" means it runs once per stack deploy/restart.
  init-sysctl:
    image: busybox:latest
    privileged: true
    command: >
      sh -c "
        sysctl -w vm.max_map_count=1048576 &&
        echo 'vm.max_map_count set to 1048576'
      "
    restart: "no"
    networks:
      - openmemory-net

  # ── 1. Memgraph MAGE ───────────────────────────────────────────────────────
  memgraph:
    image: memgraph/memgraph-mage:3.3
    restart: unless-stopped
    depends_on:
      init-sysctl:
        condition: service_completed_successfully
    networks:
      - openmemory-net
    ports:
      - "${MEMGRAPH_PORT_BOLT:-7687}:7687"   # Bolt — neo4j-driver
      - "${MEMGRAPH_PORT_LAB:-7444}:7444"    # Memgraph Lab UI (optional)
    volumes:
      - memgraph-data:/var/lib/memgraph
      - memgraph-logs:/var/log/memgraph
    command:
      - --storage-properties-on-edges=true    # edge props: SUPERSEDES.at, HAS_CATEGORY.assignedAt, etc.
      - --experimental-enabled=text-search    # BM25 full-text search (Spec 02, v3.3)
      - --storage-snapshot-interval-sec=300   # snapshot every 5 min
      - --storage-snapshot-on-exit=true       # clean snapshot on SIGTERM
      - --storage-snapshot-retention-count=3  # keep last 3 snapshots
      - --storage-wal-enabled=true            # write-ahead log (crash safety)
      - --log-level=WARNING                   # reduce MAGE Python PyTorch noise
    healthcheck:
      # mgconsole is available in the memgraph-mage image — reliable Bolt health check
      test: ["CMD-SHELL", "echo 'RETURN 1;' | mgconsole --no-history > /dev/null 2>&1 && echo ok || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"
    # Resource limits — generous for 128 GB RAM host.
    # Memgraph keeps the full graph in RAM; raise memgraph-data limit if your
    # graph grows beyond a few GB.
    deploy:
      resources:
        reservations:
          memory: 512m
        limits:
          memory: 64g       # hard ceiling — raise to 64g if needed

  # ── 2. OpenMemory Next.js app ──────────────────────────────────────────────
  openmemory:
    image: mem0/openmemory:latest
    # If using Portainer git integration with auto-build, replace `image` with:
    # build:
    #   context: ui/
    #   dockerfile: Dockerfile
    restart: unless-stopped
    depends_on:
      memgraph:
        condition: service_healthy
    networks:
      - openmemory-net
    ports:
      - "${OPENMEMORY_PORT:-3000}:3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_USER_ID=${OPENMEMORY_USER_ID:-user}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MEMGRAPH_URL=bolt://memgraph:7687
      - MEMGRAPH_USER=
      - MEMGRAPH_PASSWORD=
      # Optional Azure LLM overrides
      - LLM_AZURE_OPENAI_API_KEY=${LLM_AZURE_OPENAI_API_KEY:-}
      - LLM_AZURE_ENDPOINT=${LLM_AZURE_ENDPOINT:-}
      - LLM_AZURE_DEPLOYMENT=${LLM_AZURE_DEPLOYMENT:-}
      # Optional Azure embedding overrides
      - EMBEDDING_AZURE_OPENAI_API_KEY=${EMBEDDING_AZURE_OPENAI_API_KEY:-}
      - EMBEDDING_AZURE_ENDPOINT=${EMBEDDING_AZURE_ENDPOINT:-}
    volumes:
      - openmemory-data:/app/data
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3000/ > /dev/null 2>&1 && echo ok || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "3"
    deploy:
      resources:
        reservations:
          memory: 256m
        limits:
          memory: 2g
